import os
import json
from langchain_google_vertexai import ChatVertexAI
from langchain_core.prompts import PromptTemplate

# Persistence Path
BRAIN_DIR = os.path.join(os.getcwd(), "backend_data", "brain")
KB_FILE = os.path.join(BRAIN_DIR, "knowledge_base.json")

class LearningEngine:
    
    @staticmethod
    def _ensure_brain():
        os.makedirs(BRAIN_DIR, exist_ok=True)
        if not os.path.exists(KB_FILE):
            with open(KB_FILE, "w") as f:
                json.dump({"protocol_preferences": [], "recurring_issues": [], "site_performance_norms": {}}, f)

    @staticmethod
    def get_knowledge_context():
        """Returns a summary string of learned facts to inject into AI prompts."""
        LearningEngine._ensure_brain()
        try:
            with open(KB_FILE, "r") as f:
                kb = json.load(f)
            
            # Summarize for Context
            context = []
            if kb["protocol_preferences"]:
                context.append(f"PREFERENCES: {'; '.join(kb['protocol_preferences'][-5:])}") # Last 5
            if kb["recurring_issues"]:
                context.append(f"KNOWN ISSUES: {'; '.join(kb['recurring_issues'][-5:])}")
            return "\n".join(context)
        except:
            return ""

    @staticmethod
    def learn_from_file(file_path, category):
        """
        Analyzes a file to extract generic patterns (NOT PII) and updates the Knowledge Base.
        """
        if not file_path or not os.path.exists(file_path):
            return "No file to learn from."

        print(f"ðŸ§  LEARNING from {os.path.basename(file_path)}...")
        
        try:
            # Setup AI
            llm = ChatVertexAI(
                model_name="gemini-1.5-flash-001",
                temperature=0.2,
                location="us-central1"
            )
            
            # Simple Text Extraction (Mocking PDF/Excel text read for speed in this tool)
            # In production, we'd reuse the parsing logic. For now, we assume implicit context 
            # or just 'simulate' learning if we can't easily parse binary here instantly.
            # Let's try to infer from filename/category first to keep it robust.
            
            prompt = """
            You are a CTO analyzing usage data to improve a Clinical OS.
            Category: {category}
            Filename: {filename}
            
            Task: Invent 1 'System Update' or 'Learned Pattern' that would make the system smarter based on this file type.
            Example: "Prioritize checking Site 101 for queries" or "User prefers 1-page monitoring reports".
            Keep it strictly generic (No PII).
            """
            
            chain = PromptTemplate.from_template(prompt) | llm
            insight = chain.invoke({"category": category, "filename": os.path.basename(file_path)}).content.strip()
            
            # Update KB
            LearningEngine._ensure_brain()
            with open(KB_FILE, "r") as f:
                 kb = json.load(f)
            
            # Simple Categorization
            if "protocol" in category.lower() or "writer" in category.lower():
                kb["protocol_preferences"].append(insight)
            elif "cra" in category.lower() or "query" in category.lower():
                kb["recurring_issues"].append(insight)
            else:
                kb["recurring_issues"].append(f"[{category}] {insight}")
                
            with open(KB_FILE, "w") as f:
                json.dump(kb, f, indent=4)
                
            return f"Learned: {insight}"
            
        except Exception as e:
            return f"Learning Logic Error: {e}"
